{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 07:14:42.021961: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 18.53GiB (19901340057 bytes) by rematerialization; only reduced to 39.46GiB (42366664720 bytes), down from 39.46GiB (42366664720 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 time: 0.192535 seconds\n",
      "float16 time: 0.124722 seconds\n",
      "bfloat16 time: 0.093090 seconds\n",
      "bfloat16 is faster by 0.099445 seconds\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "from flax import linen as nn\n",
    "\n",
    "# Function to benchmark matrix multiplication\n",
    "def benchmark_matmul(dtype, size=(1024, 1024)):\n",
    "    # Generate random matrices of the given dtype\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    A = jax.random.normal(key, size, dtype=dtype)\n",
    "    B = jax.random.normal(key, size, dtype=dtype)\n",
    "\n",
    "    # JIT compile the matrix multiplication for speed\n",
    "    matmul_fn = jax.jit(lambda x, y: jnp.dot(x, y))\n",
    "\n",
    "    # Warm-up to ensure accurate timing\n",
    "    matmul_fn(A, B).block_until_ready()\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "    matmul_fn(A, B).block_until_ready()\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "# Matrix size\n",
    "matrix_size = (100, 1024, 1024)\n",
    "\n",
    "# Benchmark float32\n",
    "f32_time = benchmark_matmul(jnp.float32, size=matrix_size)\n",
    "print(f\"float32 time: {f32_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark float16\n",
    "f16_time = benchmark_matmul(jnp.float16, size=matrix_size)\n",
    "print(f\"float16 time: {f16_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "bf16_time = benchmark_matmul(jnp.bfloat16, size=matrix_size)\n",
    "print(f\"bfloat16 time: {bf16_time:.6f} seconds\")\n",
    "\n",
    "# Print the comparison\n",
    "if bf16_time < f32_time:\n",
    "    print(f\"bfloat16 is faster by {f32_time - bf16_time:.6f} seconds\")\n",
    "else:\n",
    "    print(f\"float32 is faster by {bf16_time - f32_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 time: 0.004519 seconds\n",
      "bfloat16 time: 0.002950 seconds\n",
      "float16 time: 0.002773 seconds\n",
      "bfloat16 takes 0.652896x time\n"
     ]
    }
   ],
   "source": [
    "# Function to benchmark a simple MLP network\n",
    "def benchmark_mlp(dtype, input_size=(1024, 512), hidden_size=512, output_size=256, num_layers=200):\n",
    "    # Generate random input of the given dtype\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    inputs = jax.random.normal(key, input_size, dtype=dtype)\n",
    "\n",
    "    # Initialize random weights for the MLP\n",
    "    params = []\n",
    "    in_features = input_size[1]\n",
    "    for _ in range(num_layers):\n",
    "        W = jax.random.normal(key, (in_features, hidden_size), dtype=dtype)\n",
    "        b = jax.random.normal(key, (hidden_size,), dtype=dtype)\n",
    "        params.append((W, b))\n",
    "        in_features = hidden_size\n",
    "    W_out = jax.random.normal(key, (hidden_size, output_size), dtype=dtype)\n",
    "    b_out = jax.random.normal(key, (output_size,), dtype=dtype)\n",
    "    params.append((W_out, b_out))\n",
    "\n",
    "    # Define the MLP forward function\n",
    "    def mlp_forward(x, params):\n",
    "        for W, b in params[:-1]:\n",
    "            x = jnp.dot(x, W) + b\n",
    "            x = jax.nn.relu(x)\n",
    "        W_out, b_out = params[-1]\n",
    "        return jnp.dot(x, W_out) + b_out\n",
    "\n",
    "    # JIT compile the MLP function\n",
    "    mlp_fn = jax.jit(lambda x: mlp_forward(x, params))\n",
    "\n",
    "    # Warm-up to ensure accurate timing\n",
    "    mlp_fn(inputs).block_until_ready()\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "    mlp_fn(inputs).block_until_ready()\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "# Input size\n",
    "input_size = (1024, 512)\n",
    "\n",
    "# Benchmark float32\n",
    "f32_time = benchmark_mlp(jnp.float32, input_size=input_size)\n",
    "print(f\"float32 time: {f32_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "bf16_time = benchmark_mlp(jnp.bfloat16, input_size=input_size)\n",
    "print(f\"bfloat16 time: {bf16_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "f16_time = benchmark_mlp(jnp.float16, input_size=input_size)\n",
    "print(f\"float16 time: {f16_time:.6f} seconds\")\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"bfloat16 takes {bf16_time / f32_time:.6f}x time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3545979/204634599.py:27: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  params = jax.tree_map(lambda x: x.astype(dtype), params)  # Convert to the given dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 time: 0.004996 seconds\n",
      "bfloat16 time: 0.003548 seconds\n",
      "float16 time: 0.003498 seconds\n",
      "bfloat16 takes 0.710175x time\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP model using Flax\n",
    "class MLP(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    num_layers: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nn.Dense(self.hidden_size)(x)\n",
    "            x = nn.relu(x)\n",
    "            x = nn.LayerNorm(epsilon=1e-4, use_bias=False, use_scale=False)(x)\n",
    "            \n",
    "        x = nn.Dense(self.output_size)(x)    \n",
    "        return x\n",
    "\n",
    "# Function to benchmark a simple MLP network\n",
    "def benchmark_mlp_with_flax(dtype, input_size=(1024, 512), hidden_size=512, output_size=256, num_layers=200):\n",
    "    # Generate random input of the given dtype\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    inputs = jax.random.normal(key, input_size, dtype=dtype)\n",
    "\n",
    "    # Initialize the MLP model\n",
    "    model = MLP(hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "    params = model.init(key, inputs)  # Initialize parameters\n",
    "\n",
    "    params = jax.tree_map(lambda x: x.astype(dtype), params)  # Convert to the given dtype\n",
    "\n",
    "    # JIT compile the MLP function\n",
    "    mlp_fn = jax.jit(lambda x: model.apply(params, x))\n",
    "\n",
    "    # Warm-up to ensure accurate timing\n",
    "    mlp_fn(inputs).block_until_ready()\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "    mlp_fn(inputs).block_until_ready()\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "# Input size\n",
    "input_size = (1024, 512)\n",
    "\n",
    "# Benchmark float32\n",
    "f32_time = benchmark_mlp_with_flax(jnp.float32, input_size=input_size)\n",
    "print(f\"float32 time: {f32_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "bf16_time = benchmark_mlp_with_flax(jnp.bfloat16, input_size=input_size)\n",
    "print(f\"bfloat16 time: {bf16_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark float16\n",
    "f16_time = benchmark_mlp_with_flax(jnp.float16, input_size=input_size)\n",
    "print(f\"float16 time: {f16_time:.6f} seconds\")\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"bfloat16 takes {bf16_time / f32_time:.6f}x time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple loss function\n",
    "def loss_fn(params, model, inputs, targets):\n",
    "    predictions = model.apply(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Function to benchmark a simple MLP network with gradient updates\n",
    "def benchmark_mlp_with_grad_updates(dtype, input_size=(1024, 512), hidden_size=512, output_size=256, num_layers=200, num_updates=100):\n",
    "    # Generate random input and targets of the given dtype\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key1, key2, key3 = jax.random.split(key, 3)\n",
    "    inputs = jax.random.normal(key1, input_size, dtype=dtype)\n",
    "    targets = jax.random.normal(key2, (input_size[0], output_size), dtype=dtype)\n",
    "\n",
    "    # Initialize the MLP model\n",
    "    model = MLP(hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "    params = model.init(key3, inputs)  # Initialize parameters\n",
    "    params = jax.tree_map(lambda x: x.astype(dtype), params)  # Convert to the given dtype\n",
    "\n",
    "    # Define the gradient update function\n",
    "    def update(params, inputs, targets):\n",
    "        grads = jax.grad(loss_fn)(params, model, inputs, targets)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        new_params = jax.tree_util.tree_map(lambda p, g: p - 0.01 * g, params, grads)\n",
    "        # new_params = params\n",
    "        return grads, new_params\n",
    "\n",
    "    # JIT compile the update function\n",
    "    update_fn = jax.jit(update)\n",
    "\n",
    "    # This should include some bfloat16 casts to float32 because we applied layernorm\n",
    "    # print(jax.make_jaxpr(update_fn)(params, inputs, targets))\n",
    "\n",
    "    # Warm-up to ensure accurate timing\n",
    "    grads, params = update_fn(params, inputs, targets)\n",
    "\n",
    "    # Measure execution time for multiple updates\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_updates):\n",
    "        grads, params = update_fn(params, inputs, targets)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "# Input size\n",
    "input_size = (1024, 512)\n",
    "\n",
    "# This makes no difference\n",
    "# jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\")\n",
    "\n",
    "# Benchmark float32\n",
    "f32_time = benchmark_mlp_with_grad_updates(jnp.float32, input_size=input_size)\n",
    "print(f\"float32 time: {f32_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "f16_time = benchmark_mlp_with_grad_updates(jnp.float16, input_size=input_size)\n",
    "print(f\"float16 time: {f16_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark bfloat16\n",
    "bf16_time = benchmark_mlp_with_grad_updates(jnp.bfloat16, input_size=input_size)\n",
    "print(f\"bfloat16 time: {bf16_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"bfloat16 takes {bf16_time / f32_time:.6f}x time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
