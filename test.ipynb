{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepares the input pipeline for OpenWebText (OWT).\n",
    "\n",
    "This script tokenizes the OWT dataset and splits it into train and eval sets.\n",
    "The train and eval sets are saved as ArrayRecord files.\n",
    "\"\"\"\n",
    "\n",
    "# from array_record.python import array_record_module\n",
    "import datasets\n",
    "\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tqdm\n",
    "import transformers\n",
    "\n",
    "# import time\n",
    "\n",
    "\n",
    "source = datasets.load_dataset(\n",
    "    \"Skylion007/openwebtext\", name=\"plain_text\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "_GPT2_TOKENIZER = \"gpt2\"\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(_GPT2_TOKENIZER)\n",
    "\n",
    "# ArrayRecordWriter = array_record_module.ArrayRecordWriter\n",
    "# ArrayRecordReader = array_record_module.ArrayRecordReader\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "ds_output_file_train = \"./data_dir/openwebtext_splits_1024_train\"\n",
    "ds_output_file_eval = \"./data_dir/openwebtext_splits_1024_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chunk(chunk):\n",
    "    return tokenizer(chunk)\n",
    "\n",
    "\n",
    "# Main function\n",
    "def parallel_tokenize(strings, chunk_size=1000, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    Tokenizes a large list of strings in parallel.\n",
    "\n",
    "    Args:\n",
    "        strings (list of str): The list of strings to tokenize.\n",
    "        chunk_size (int): The size of each chunk for processing.\n",
    "        num_jobs (int): The number of parallel jobs (-1 uses all CPUs).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with tokenized results combined across all chunks.\n",
    "    \"\"\"\n",
    "    # Split data into chunks\n",
    "    chunks = [strings[i : i + chunk_size] for i in range(0, len(strings), chunk_size)]\n",
    "\n",
    "    # Tokenize each chunk in parallel\n",
    "    tokenized_chunks = Parallel(n_jobs=num_jobs)(\n",
    "        delayed(tokenize_chunk)(chunk) for chunk in chunks\n",
    "    )\n",
    "\n",
    "    return tokenized_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024  # size of the chunk\n",
    "\n",
    "data_iter = iter(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51193it [00:30, 3546.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1846 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1253 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1493 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2540 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1315 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6728 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1506 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2448 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3284 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2959 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1651 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1455 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1661 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1624 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1976 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1320 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1356 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2046 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1843 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1409 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2296 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1904 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1533 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9417 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1574 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3609 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1573 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2099 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2369 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4746 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2887 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4728 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1632 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1551 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2531 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2938 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1764 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1926 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1691 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11738 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2322 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1643 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1265 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7159 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1476 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8559 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1920 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1893 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1471 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1701 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1841 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16628 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3214 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1272 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1528 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1622 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1578 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7712 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1667 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1081 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2339 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1530 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3238 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1986 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1815 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2818 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3720 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3205 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1318 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1729 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1404 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1566 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1391 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1416 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1372 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2566 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1611 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1618 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7387 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3534 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1315 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1379 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1775 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2313 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2566 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1402 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1704 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2327 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1345 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3574 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1311 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1739 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1823 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1394 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1823 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4220 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1587 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1651 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2256 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3602 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1364 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2046 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1980 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5152 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1503 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1620 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1390 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1683 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3726 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1435 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1835 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2052 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1396 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1608 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1430 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1741 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1170 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1970 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3473 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9336 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3314 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7223 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3899 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1354 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2294 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1293 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1810 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1254 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3216 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2682 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1356 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5209 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1328 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1316 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1535 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1869 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4574 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5133 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1458 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5509 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1417 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1911 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2235 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1258 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1281 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1929 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6250 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1855 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1099 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2175 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1530 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1413 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1369 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1742 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2359 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1545 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1510 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2116 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2346 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2809 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1443 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1410 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2173 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1611 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3260 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1348 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3536 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4211 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1566 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1178 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1110 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1366 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1518 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1456 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1800 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1371 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1862 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1782 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1147 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1969 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1606 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1288 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1314 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1726 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1753 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1599 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3073 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1397 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1092 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2354 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1437 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1875 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3952 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1669 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3327 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1555 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1486 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1550 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1970 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1257 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1477 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1841 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1531 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2903 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2479 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2637 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2471 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1830 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1439 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1287 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5355 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1468 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1715 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1207 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2530 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1899 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1510 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1457 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1526 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1774 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10583 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13660 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1681 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1711 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2664 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1786 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1558 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1438 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2653 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1172 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2937 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2191 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2384 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1583 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3502 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1444 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10252 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1331 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3166 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1688 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1509 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9119 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1249 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16520 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3629 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2596 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1654 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1595 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3775 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3625 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2023 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2438 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8995 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1811 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1469 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2675 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2521 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2087 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6147 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1482 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1129 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2881 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2781 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1790 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1249 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1256 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1925 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1776 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1658 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1513 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3416 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7053 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1688 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3284 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1229 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1725 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1714 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2601 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1747 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1916 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1701 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2616 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1615 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1934 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1271 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1474 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2146 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2864 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1862 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1526 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5579 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1618 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1709 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1506 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3830 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2472 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9402 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1279 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2347 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4203 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3239 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2708 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1754 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1126 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3234 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1767 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1234 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5840 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2256 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2143 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2898 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20467 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4441 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3478 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1316 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3773 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1404 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1805 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1170 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3883 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1272 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1375 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2204 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2779 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1616 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8586 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2464 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1684 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8036 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3003 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3591 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3982 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "51200it [01:09, 734.62it/s] \n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "count = 0\n",
    "count_per_save = 0\n",
    "eval_chunks = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_list = []\n",
    "for i, example in tqdm(enumerate(data_iter)):\n",
    "    text_list.append(example[\"text\"])\n",
    "    if i >= 100 * 512:\n",
    "        tokenized_result = parallel_tokenize(text_list, chunk_size=100, num_jobs=512)\n",
    "        break\n",
    "    continue\n",
    "    tokens = tokenizer(example[\"text\"])[\"input_ids\"]\n",
    "    all_tokens.extend(tokens + [tokenizer.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_result = parallel_tokenize(text_list, chunk_size=1000, num_jobs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\\n\\nThe decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\\n\\nCNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the Belgian First Aid and Support Team to evacuate. However, Belgian Chief Coordinator Geert Gijs, a doctor who was at the hospital with 60 Belgian medical personnel, said it was his decision to pull the team out for the night. Gijs said he requested U.N. security personnel to staff the hospital overnight, but was told that peacekeepers would only be able to evacuate the team.\\n\\nHe said it was a \"tough decision\" but that he accepted the U.N. offer to evacuate after a Canadian medical team, also at the hospital with Canadian security officers, left the site Friday afternoon. The Belgian team returned Saturday morning.\\n\\nGijs said the United Nations has agreed to provide security for Saturday night. The team has requested the Belgian government to send its own troops for the field hospital, which Gijs expects to arrive late Sunday.\\n\\nResponding to the CNN report that Gupta was the only doctor left at the Port-au-Prince field hospital, U.N. spokesman Martin Nesirky said Saturday that the world body\\'s mission in Haiti did not order any medical team to leave. If the team left, it was at the request of their own organization, he said.\\n\\nEdmond Mulet, the U.N. assistant secretary general for peacekeeping operations, told reporters later that local security officers deemed the makeshift hospital unsafe.\\n\\n\"It seems that we\\'ve heard some reports in the international media that the United Nations asked or forced some medical teams to not work any more in some clinic -- that is not true, that is completely untrue,\" Mulet said Saturday.\\n\\nCNN video from the scene Friday night shows the Belgian team packing up its supplies and leaving with an escort of blue-helmeted U.N. peacekeepers in marked trucks.\\n\\nView or add to CNN\\'s database of missing persons in Haiti\\n\\nGupta -- assisted by other CNN staffers, security personnel and at least one Haitian nurse who refused to leave -- assessed the needs of the 25 patients, but there was little they could do without supplies.\\n\\nMore people, some in critical condition, were trickling in late Friday.\\n\\n\"I\\'ve never been in a situation like this. This is quite ridiculous,\" Gupta said.\\n\\nWith a dearth of medical facilities in Haiti\\'s capital, ambulances had nowhere else to take patients, some of whom had suffered severe trauma -- amputations and head injuries -- under the rubble. Others had suffered a great deal of blood loss, but there were no blood supplies left at the clinic.\\n\\nGupta feared that some would not survive the night.\\n\\nHe and the others stayed with the injured all night, after the medical team had left and after the generators gave out and the tents turned pitch black.\\n\\nGupta monitored patients\\' vital signs, administered painkillers and continued intravenous drips. He stabilized three new patients in critical condition.\\n\\nAt 3:45 a.m., he posted a message on Twitter: \"pulling all nighter at haiti field hosp. lots of work, but all patients stable. turned my crew into a crack med team tonight.\"\\n\\nAre you in Haiti and safe? Share your photos\\n\\nHe said the Belgian doctors did not want to leave their patients behind but were ordered out by the United Nations, which sent buses to transport them.\\n\\n\"There is concern about riots not far from here -- and this is part of the problem,\" Gupta said.\\n\\nThere have been scattered reports of violence throughout the capital.\\n\\n\"What is striking to me as a physician is that patients who just had surgery, patients who are critically ill, are essentially being left here, nobody to care for them,\" Gupta said.\\n\\nSandra Pierre, a Haitian who has been helping at the makeshift hospital, said the medical staff took most of the supplies with them.\\n\\n\"All the doctors, all the nurses are gone,\" she said. \"They are expected to be back tomorrow. They had no plan on leaving tonight. It was an order that came suddenly.\"\\n\\nShe told Gupta, \"It\\'s just you.\"\\n\\nA 7.0 magnitude earthquake flattened Haiti\\'s capital city Tuesday afternoon, affecting as many as 3 million people as it fanned out across the island nation. Tens of thousands of people are feared dead.\\n\\nHaiti, the poorest nation in the Western hemisphere, lacked adequate medical resources even before the disaster and has been struggling this week to tend to huge numbers of injured. The clinic, set up under several tents, was a godsend to the few who were lucky to have been brought there.\\n\\nRetired Army Lt. Gen. Russel Honore, who led relief efforts for Hurricane Katrina in 2005, said the evacuation of the clinic\\'s medical staff was unforgivable.\\n\\n\"Search and rescue must trump security,\" Honoré said. \"I\\'ve never seen anything like this before in my life. They need to man up and get back in there.\"\\n\\nHonoré drew parallels between the tragedy in New Orleans, Louisiana, and in Port-au-Prince. But even in the chaos of Katrina, he said, he had never seen medical staff walk away.\\n\\n\"I find this astonishing these doctors left,\" he said. \"People are scared of the poor.\"\\n\\nCNN\\'s Justine Redman, Danielle Dellorto and John Bonifield contributed to this report.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "_GPT2_TOKENIZER = \"gpt2\"\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(_GPT2_TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21/21 [00:00<00:00, 403.05files/s]\n",
      "Generating train split:   1%|          | 71276/8013769 [00:15<27:53, 4744.70 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m NUM_WORKERS \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcpu_count()  \u001b[38;5;66;03m# Number of workers (adjust as needed)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset (streaming mode)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSkylion007/openwebtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplain_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_chunk\u001b[39m(chunk_range):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Tokenize a chunk of data using skip and take for streaming datasets.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        list of dict: Tokenized data.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/builder.py:1486\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1484\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1487\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1488\u001b[0m     ):\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1490\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/builder.py:1624\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     shard_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1615\u001b[0m     writer \u001b[38;5;241m=\u001b[39m writer_class(\n\u001b[1;32m   1616\u001b[0m         features\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39m_features,\n\u001b[1;32m   1617\u001b[0m         path\u001b[38;5;241m=\u001b[39mfpath\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSSSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJJJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         embed_local_files\u001b[38;5;241m=\u001b[39membed_local_files,\n\u001b[1;32m   1623\u001b[0m     )\n\u001b[0;32m-> 1624\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n\u001b[1;32m   1625\u001b[0m writer\u001b[38;5;241m.\u001b[39mwrite(example, key)\n\u001b[1;32m   1626\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/features/features.py:1993\u001b[0m, in \u001b[0;36mFeatures.encode_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;124;03mEncode example into a format for Arrow.\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;124;03m    `dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m example \u001b[38;5;241m=\u001b[39m cast_to_python_objects(example)\n\u001b[0;32m-> 1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/features/features.py:1282\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot None but expected a dictionary instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1282\u001b[0m         {k: encode_nested_example(schema[k], obj\u001b[38;5;241m.\u001b[39mget(k), level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema}\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1288\u001b[0m     sub_schema \u001b[38;5;241m=\u001b[39m schema[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/features/features.py:1282\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot None but expected a dictionary instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1282\u001b[0m         {k: \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema}\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1288\u001b[0m     sub_schema \u001b[38;5;241m=\u001b[39m schema[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/features/features.py:1352\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD, Video)):\n\u001b[0;32m-> 1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/lean-dojo/lib/python3.10/site-packages/datasets/features/features.py:525\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_boolean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_type\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_integer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_type):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import GPT2Tokenizer\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "_GPT2_TOKENIZER = \"gpt2\"\n",
    "CHUNK_SIZE = 100\n",
    "NUM_WORKERS = os.cpu_count()  # Number of workers (adjust as needed)\n",
    "\n",
    "# Load the dataset (streaming mode)\n",
    "source = datasets.load_dataset(\n",
    "    \"Skylion007/openwebtext\", name=\"plain_text\", split=\"train\"\n",
    ")\n",
    "\n",
    "\n",
    "def process_chunk(chunk_range):\n",
    "    \"\"\"\n",
    "    Tokenize a chunk of data using skip and take for streaming datasets.\n",
    "\n",
    "    Args:\n",
    "        chunk_range (tuple): Start and end indices for the chunk.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Tokenized data.\n",
    "    \"\"\"\n",
    "    start, end = chunk_range\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(_GPT2_TOKENIZER)\n",
    "\n",
    "    # Efficiently access the desired range\n",
    "    chunk = source.skip(start).take(end - start)\n",
    "    tokenized_data = [tokenizer(text[\"text\"]) for text in chunk]\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "def create_chunks(total_size, chunk_size):\n",
    "    \"\"\"Create chunk ranges for parallel processing.\"\"\"\n",
    "    return [\n",
    "        (i, min(i + chunk_size, total_size)) for i in range(0, total_size, chunk_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "def save_tokenized_data(tokenized_data, output_file):\n",
    "    \"\"\"Save tokenized data to a file.\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for item in tokenized_data:\n",
    "            f.write(str(item) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_size = 100000  # Adjust to the number of samples in your dataset\n",
    "    chunks = create_chunks(total_size, CHUNK_SIZE)\n",
    "\n",
    "    # Run parallel tokenization\n",
    "    with Pool(NUM_WORKERS) as pool:\n",
    "        tokenized_results = pool.map(process_chunk, chunks)\n",
    "\n",
    "    # Combine and save results\n",
    "    all_tokenized_data = [item for sublist in tokenized_results for item in sublist]\n",
    "    # save_tokenized_data(all_tokenized_data, \"./data_dir/openwebtext_tokenized.txt\")\n",
    "    import pickle\n",
    "\n",
    "    output_pickle_file = \"./data_dir/openwebtext_tokenized.pkl\"\n",
    "    with open(output_pickle_file, \"wb\") as f:\n",
    "        pickle.dump(all_tokenized_data, f)\n",
    "\n",
    "    print(f\"Tokenization complete. Total tokenized entries: {len(all_tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = load_from_disk(\"./openwebtext_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 8013769\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fecf6ca1db0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "output_pickle_file = \"./openwebtext_tokenized.pkl\"\n",
    "with open(output_pickle_file, \"rb\") as f:\n",
    "    all_tokenized_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_data_input_ids = [data[\"input_ids\"] for data in all_tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokenized_data_input_ids[-2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "output_pickle_file = \"./openwebtext_tokenized_inputids.pkl\"\n",
    "with open(output_pickle_file, \"rb\") as f:\n",
    "    all_tokenized_data_input_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8013769"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokenized_data_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 8013769/8013769 [03:41<00:00, 36182.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "output_pickle_file = \"./openwebtext_tokenized_inputids.pkl\"\n",
    "with open(output_pickle_file, \"rb\") as f:\n",
    "    all_tokenized_data_input_ids = pickle.load(f)\n",
    "\n",
    "# ArrayRecordWriter = array_record_module.ArrayRecordWriter\n",
    "# ArrayRecordReader = array_record_module.ArrayRecordReader\n",
    "\n",
    "\n",
    "# def _int64_feature(value):\n",
    "#     \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "#     return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "_GPT2_TOKENIZER = \"gpt2\"\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(_GPT2_TOKENIZER)\n",
    "\n",
    "ds_output_file_train = \"./data_dir/openwebtext_splits_1024_train\"\n",
    "ds_output_file_eval = \"./data_dir/openwebtext_splits_1024_eval\"\n",
    "\n",
    "n_examples = 8013769  # tiny: 2; small: 10_000; full: 8013769\n",
    "save_every_examples = 100_000\n",
    "block_size = 1024  # size of the chunk\n",
    "\n",
    "# data_iter = iter(source)\n",
    "\n",
    "all_tokens = []\n",
    "count = 0\n",
    "count_per_save = 0\n",
    "eval_chunks = []\n",
    "\n",
    "# writer_train = ArrayRecordWriter(ds_output_file_train, \"group_size:1\")\n",
    "# writer_eval = ArrayRecordWriter(ds_output_file_eval, \"group_size:1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for tokens in tqdm(all_tokenized_data_input_ids):\n",
    "    # tokens = tokenizer(example[\"text\"])[\"input_ids\"]\n",
    "    all_tokens.extend(tokens + [tokenizer.eos_token_id])\n",
    "    count += 1\n",
    "    count_per_save += 1\n",
    "    \n",
    "all_tokens = np.array(all_tokens)\n",
    "np.save(\"openwebtext_tokenized_inputids.npy\", all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = np.array(all_tokens)\n",
    "np.save(\"openwebtext_tokenized_inputids.npy\", all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_tokens = np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_length = (len(all_tokens) // block_size) * block_size\n",
    "all_tokens_short = all_tokens[:saved_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_short=all_tokens_short.reshape(-1, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = all_tokens_short\n",
    "np.random.shuffle(chunks)\n",
    "num_eval = int(len(chunks) * 0.02)  # put 2% of chunks into eval split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chunks = chunks[:num_eval]\n",
    "train_chunks = chunks[num_eval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./data_dir/openwebtext_np_train.npy\", train_chunks)\n",
    "# np.save(\"./data_dir/openwebtext_np_eval.npy\", eval_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  257,  1256,   286, ..., 26318,  1757,   371])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 389, 1016,  284, ...,   64,  319, 3909])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x79ea1752ddb0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "saved_length = (len(all_tokens) // block_size) * block_size\n",
    "chunks = [\n",
    "    all_tokens[i : i + block_size] for i in tqdm(range(0, saved_length, block_size))\n",
    "]\n",
    "\n",
    "# print(\"Time taken to tokenize:\", time.time() - time1)\n",
    "print(f\"\\nsaving chunks @ {count}th example mark...\")\n",
    "np.random.shuffle(chunks)\n",
    "num_eval = int(len(chunks) * 0.02)  # put 2% of chunks into eval split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/md4-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-18 08:08:44.027518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737187724.068846 1569184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737187724.080157 1569184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 08:08:44.113937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from array_record.python import array_record_module\n",
    "import datasets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import transformers\n",
    "import time\n",
    "import os\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_GPT2_TOKENIZER = \"gpt2\"\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(_GPT2_TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8013769 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 96461/8013769 [00:02<03:07, 42165.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.49033784866333\n",
      "\n",
      "saving chunks @ 100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2206/2206 [00:00<00:00, 3557.20it/s]\n",
      "100%|██████████| 108109/108109 [00:34<00:00, 3173.37it/s]]\n",
      "  1%|▏         | 108634/8013769 [00:40<2:56:11, 747.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.19849228858948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 196296/8013769 [00:42<03:34, 36501.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.535370111465454\n",
      "\n",
      "saving chunks @ 200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2224/2224 [00:00<00:00, 3560.31it/s]\n",
      "100%|██████████| 109011/109011 [00:34<00:00, 3134.93it/s]s]\n",
      "  3%|▎         | 209135/8013769 [01:21<2:59:27, 724.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.94989895820618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 299868/8013769 [01:24<03:36, 35708.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.1835684776306152\n",
      "\n",
      "saving chunks @ 300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [00:00<00:00, 3185.07it/s]\n",
      "100%|██████████| 108649/108649 [00:35<00:00, 3019.57it/s]s]\n",
      "  4%|▍         | 306737/8013769 [02:05<4:13:03, 507.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 39.8833065032959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 396846/8013769 [03:44<03:22, 37606.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 97.51064085960388\n",
      "\n",
      "saving chunks @ 400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2218/2218 [00:00<00:00, 3443.15it/s]\n",
      "100%|██████████| 108699/108699 [00:36<00:00, 2998.53it/s]\n",
      "  5%|▌         | 406821/8013769 [04:22<11:52:04, 178.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 134.4242742061615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 497283/8013769 [04:25<03:33, 35227.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.538282632827759\n",
      "\n",
      "saving chunks @ 500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2194/2194 [00:00<00:00, 3300.96it/s]\n",
      "100%|██████████| 107534/107534 [00:34<00:00, 3127.00it/s]s]\n",
      "  6%|▋         | 507560/8013769 [05:03<3:15:22, 640.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.610230445861816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 596452/8013769 [05:06<03:27, 35828.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.514292001724243\n",
      "\n",
      "saving chunks @ 600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [00:00<00:00, 2716.80it/s]\n",
      "100%|██████████| 107621/107621 [00:33<00:00, 3180.31it/s]s]\n",
      "  8%|▊         | 607459/8013769 [05:44<3:06:46, 660.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.1840717792511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 699881/8013769 [05:47<03:45, 32483.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.5763652324676514\n",
      "\n",
      "saving chunks @ 700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2214/2214 [00:00<00:00, 3262.33it/s]\n",
      "100%|██████████| 108507/108507 [00:33<00:00, 3222.50it/s]s]\n",
      "  9%|▉         | 705610/8013769 [06:25<4:19:59, 468.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.946409940719604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 799196/8013769 [06:28<03:12, 37569.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.368776798248291\n",
      "\n",
      "saving chunks @ 800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2186/2186 [00:00<00:00, 3577.33it/s]\n",
      "100%|██████████| 107155/107155 [00:33<00:00, 3239.00it/s]s]\n",
      " 10%|█         | 807207/8013769 [07:04<3:18:02, 606.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.08050489425659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 897943/8013769 [07:07<03:08, 37687.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.772588014602661\n",
      "\n",
      "saving chunks @ 900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [00:00<00:00, 3526.72it/s]\n",
      "100%|██████████| 107848/107848 [00:33<00:00, 3193.51it/s]s]\n",
      " 11%|█▏        | 902700/8013769 [07:45<5:03:37, 390.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.1861457824707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 999501/8013769 [07:48<03:48, 30641.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.512913703918457\n",
      "\n",
      "saving chunks @ 1000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [00:00<00:00, 3304.47it/s]\n",
      "100%|██████████| 108001/108001 [00:34<00:00, 3093.04it/s]s]\n",
      " 13%|█▎        | 1006163/8013769 [08:28<4:13:11, 461.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 39.12090468406677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1098231/8013769 [08:31<03:46, 30478.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.856701612472534\n",
      "\n",
      "saving chunks @ 1100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [00:00<00:00, 2910.67it/s]\n",
      "100%|██████████| 107814/107814 [00:36<00:00, 2960.23it/s]/s]\n",
      " 14%|█▎        | 1101820/8013769 [09:12<6:51:43, 279.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 40.055588245391846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1198155/8013769 [09:16<03:13, 35229.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.341883420944214\n",
      "\n",
      "saving chunks @ 1200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2207/2207 [00:00<00:00, 3286.65it/s]\n",
      "100%|██████████| 108182/108182 [00:33<00:00, 3276.61it/s]/s]\n",
      " 15%|█▌        | 1206578/8013769 [09:54<3:17:14, 575.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.05030298233032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1297020/8013769 [09:56<02:59, 37319.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.163818597793579\n",
      "\n",
      "saving chunks @ 1300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [00:00<00:00, 2843.82it/s]\n",
      "100%|██████████| 107646/107646 [00:33<00:00, 3231.28it/s]/s]\n",
      " 16%|█▋        | 1302666/8013769 [10:34<4:34:07, 408.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.27259039878845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1399821/8013769 [10:37<02:56, 37553.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.6891086101531982\n",
      "\n",
      "saving chunks @ 1400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [00:00<00:00, 3326.77it/s]\n",
      "100%|██████████| 107570/107570 [00:36<00:00, 2979.18it/s]/s]\n",
      " 18%|█▊        | 1402849/8013769 [11:17<5:37:36, 326.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 39.47388219833374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 1496928/8013769 [11:20<03:12, 33916.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.1439762115478516\n",
      "\n",
      "saving chunks @ 1500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 2772.26it/s]\n",
      "100%|██████████| 108470/108470 [00:33<00:00, 3233.92it/s]/s]\n",
      " 19%|█▉        | 1502597/8013769 [11:59<4:38:56, 389.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.50323796272278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1599466/8013769 [12:02<02:52, 37281.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7182204723358154\n",
      "\n",
      "saving chunks @ 1600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2208/2208 [00:00<00:00, 3303.33it/s]\n",
      "100%|██████████| 108195/108195 [00:35<00:00, 3076.17it/s]/s]\n",
      " 20%|██        | 1602971/8013769 [12:41<5:10:42, 343.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 38.5762677192688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1697452/8013769 [12:44<02:54, 36252.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.745781660079956\n",
      "\n",
      "saving chunks @ 1700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2220/2220 [00:00<00:00, 2413.06it/s]\n",
      "100%|██████████| 108780/108780 [00:33<00:00, 3277.11it/s]/s]\n",
      " 21%|██        | 1702757/8013769 [13:22<4:24:14, 398.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.87900924682617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1799248/8013769 [13:25<02:48, 36982.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.850130081176758\n",
      "\n",
      "saving chunks @ 1800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 2562.28it/s]\n",
      "100%|██████████| 107459/107459 [00:34<00:00, 3146.79it/s]/s]\n",
      " 23%|██▎       | 1803139/8013769 [14:03<4:47:09, 360.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.87693119049072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1899615/8013769 [14:06<02:48, 36374.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.324234962463379\n",
      "\n",
      "saving chunks @ 1900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2190/2190 [00:00<00:00, 3123.12it/s]\n",
      "100%|██████████| 107325/107325 [00:35<00:00, 3031.73it/s]/s]\n",
      " 24%|██▎       | 1902677/8013769 [14:45<5:12:00, 326.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 38.44513988494873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1997791/8013769 [14:48<02:42, 37000.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7004101276397705\n",
      "\n",
      "saving chunks @ 2000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2194/2194 [00:00<00:00, 3354.59it/s]\n",
      "100%|██████████| 107537/107537 [00:31<00:00, 3378.02it/s]/s]\n",
      " 25%|██▌       | 2006830/8013769 [15:24<2:38:14, 632.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.206560373306274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2099573/8013769 [15:27<02:39, 37162.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.910207986831665\n",
      "\n",
      "saving chunks @ 2100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2216/2216 [00:00<00:00, 3430.17it/s]\n",
      "100%|██████████| 108615/108615 [00:33<00:00, 3195.83it/s]/s]\n",
      " 26%|██▌       | 2102658/8013769 [16:05<4:52:46, 336.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 37.560399293899536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2197321/8013769 [16:08<02:37, 36838.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7306032180786133\n",
      "\n",
      "saving chunks @ 2200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 3321.98it/s]\n",
      "100%|██████████| 108465/108465 [00:34<00:00, 3102.82it/s]/s]\n",
      " 28%|██▊       | 2206248/8013769 [16:48<2:51:30, 564.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 38.37325167655945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2298235/8013769 [16:50<02:33, 37235.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7775516510009766\n",
      "\n",
      "saving chunks @ 2300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 3530.62it/s]\n",
      "100%|██████████| 108470/108470 [00:32<00:00, 3287.73it/s]/s]\n",
      " 29%|██▊       | 2302677/8013769 [17:27<4:05:57, 387.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.413780212402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2397782/8013769 [18:53<02:31, 37030.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 83.4878351688385\n",
      "\n",
      "saving chunks @ 2400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2222/2222 [00:00<00:00, 3198.88it/s]\n",
      "100%|██████████| 108906/108906 [00:32<00:00, 3323.21it/s]\n",
      " 30%|███       | 2410626/8013769 [19:28<6:12:11, 250.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 116.97152948379517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2495998/8013769 [19:30<02:39, 34678.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.4804131984710693\n",
      "\n",
      "saving chunks @ 2500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3039.48it/s]\n",
      "100%|██████████| 107464/107464 [00:31<00:00, 3412.54it/s]/s]\n",
      " 31%|███▏      | 2509642/8013769 [20:05<1:47:26, 853.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.71020293235779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2596591/8013769 [20:08<02:19, 38812.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.528794050216675\n",
      "\n",
      "saving chunks @ 2600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2215/2215 [00:00<00:00, 3187.58it/s]\n",
      "100%|██████████| 108576/108576 [00:32<00:00, 3303.31it/s]/s]\n",
      " 32%|███▏      | 2603408/8013769 [20:44<3:03:36, 491.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.11000418663025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 2696629/8013769 [20:47<02:24, 36908.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8296701908111572\n",
      "\n",
      "saving chunks @ 2700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2186/2186 [00:00<00:00, 3606.69it/s]\n",
      "100%|██████████| 107115/107115 [00:32<00:00, 3330.78it/s]/s]\n",
      " 34%|███▎      | 2702575/8013769 [21:24<3:25:34, 430.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.61534094810486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 2797715/8013769 [21:27<02:22, 36500.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7469143867492676\n",
      "\n",
      "saving chunks @ 2800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2197/2197 [00:00<00:00, 2995.32it/s]\n",
      "100%|██████████| 107699/107699 [00:32<00:00, 3362.26it/s]/s]\n",
      " 35%|███▍      | 2803110/8013769 [22:03<3:25:39, 422.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.53132629394531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2898786/8013769 [22:05<02:19, 36640.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7291758060455322\n",
      "\n",
      "saving chunks @ 2900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [00:00<00:00, 3096.03it/s]\n",
      "100%|██████████| 108085/108085 [00:32<00:00, 3292.61it/s]/s]\n",
      " 36%|███▌      | 2902699/8013769 [22:43<3:51:51, 367.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.28621959686279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2997318/8013769 [22:46<02:16, 36747.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.608959197998047\n",
      "\n",
      "saving chunks @ 3000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [00:00<00:00, 3247.44it/s]\n",
      "100%|██████████| 107645/107645 [00:31<00:00, 3382.04it/s]/s]\n",
      " 37%|███▋      | 3003326/8013769 [23:21<3:06:02, 448.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.130383014678955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 3099401/8013769 [23:24<02:14, 36655.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.522179126739502\n",
      "\n",
      "saving chunks @ 3100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2183/2183 [00:00<00:00, 3277.60it/s]\n",
      "100%|██████████| 107009/107009 [00:32<00:00, 3325.67it/s]/s]\n",
      " 39%|███▉      | 3105922/8013769 [24:00<2:31:28, 540.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.381640672683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3197468/8013769 [24:03<02:12, 36352.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.782078742980957\n",
      "\n",
      "saving chunks @ 3200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2201/2201 [00:00<00:00, 3157.17it/s]\n",
      "100%|██████████| 107887/107887 [00:32<00:00, 3282.17it/s]/s]\n",
      " 40%|███▉      | 3203414/8013769 [24:40<3:07:49, 426.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.36699557304382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 3298127/8013769 [24:43<02:07, 36864.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.694697380065918\n",
      "\n",
      "saving chunks @ 3300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2212/2212 [00:00<00:00, 3599.05it/s]\n",
      "100%|██████████| 108433/108433 [00:32<00:00, 3301.99it/s]/s]\n",
      " 41%|████      | 3302658/8013769 [25:20<3:23:36, 385.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.16498780250549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3397931/8013769 [25:23<02:06, 36526.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.720646381378174\n",
      "\n",
      "saving chunks @ 3400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2192/2192 [00:00<00:00, 3289.03it/s]\n",
      "100%|██████████| 107426/107426 [00:32<00:00, 3326.38it/s]/s]\n",
      " 42%|████▏     | 3403326/8013769 [26:00<3:01:29, 423.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.70170545578003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 3498055/8013769 [26:02<02:03, 36671.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.796908378601074\n",
      "\n",
      "saving chunks @ 3500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2211/2211 [00:00<00:00, 3212.23it/s]\n",
      "100%|██████████| 108341/108341 [00:32<00:00, 3325.16it/s]/s]\n",
      " 44%|████▎     | 3502788/8013769 [26:39<3:11:37, 392.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.0848662853241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 3598467/8013769 [26:42<01:59, 37078.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.6867377758026123\n",
      "\n",
      "saving chunks @ 3600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2202/2202 [00:00<00:00, 3438.75it/s]\n",
      "100%|██████████| 107942/107942 [00:32<00:00, 3302.80it/s]/s]\n",
      " 45%|████▍     | 3603535/8013769 [27:19<2:57:42, 413.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.02655816078186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 3698719/8013769 [27:22<01:57, 36595.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.756847858428955\n",
      "\n",
      "saving chunks @ 3700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [00:00<00:00, 2988.12it/s]\n",
      "100%|██████████| 108048/108048 [00:32<00:00, 3354.14it/s]/s]\n",
      " 46%|████▌     | 3702898/8013769 [27:59<3:07:40, 382.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.72539186477661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 3798908/8013769 [28:01<01:54, 36830.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.6971068382263184\n",
      "\n",
      "saving chunks @ 3800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3315.01it/s]\n",
      "100%|██████████| 107460/107460 [00:32<00:00, 3331.56it/s]/s]\n",
      " 47%|████▋     | 3803184/8013769 [28:38<2:57:24, 395.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.63102149963379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 3898055/8013769 [28:41<01:52, 36459.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.809682607650757\n",
      "\n",
      "saving chunks @ 3900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2211/2211 [00:00<00:00, 3086.92it/s]\n",
      "100%|██████████| 108355/108355 [00:32<00:00, 3367.30it/s]/s]\n",
      " 49%|████▊     | 3902594/8013769 [29:17<2:56:39, 387.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.72614312171936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 3998331/8013769 [29:20<01:48, 37037.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7227861881256104\n",
      "\n",
      "saving chunks @ 4000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [00:00<00:00, 2949.27it/s]\n",
      "100%|██████████| 107598/107598 [00:32<00:00, 3336.04it/s]/s]\n",
      " 50%|████▉     | 4003583/8013769 [29:56<2:37:10, 425.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.738200187683105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 4097543/8013769 [29:59<01:47, 36589.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7380263805389404\n",
      "\n",
      "saving chunks @ 4100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2209/2209 [00:00<00:00, 3223.14it/s]\n",
      "100%|██████████| 108251/108251 [00:32<00:00, 3333.70it/s]/s]\n",
      " 51%|█████     | 4102622/8013769 [30:36<2:42:37, 400.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.91330933570862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 4199038/8013769 [30:39<01:42, 37117.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.3793039321899414\n",
      "\n",
      "saving chunks @ 4200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2218/2218 [00:00<00:00, 3016.69it/s]\n",
      "100%|██████████| 108704/108704 [00:32<00:00, 3392.75it/s]/s]\n",
      " 52%|█████▏    | 4206780/8013769 [31:15<1:46:36, 595.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.17500400543213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 4299205/8013769 [31:18<01:38, 37688.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.3994855880737305\n",
      "\n",
      "saving chunks @ 4300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [00:00<00:00, 3054.41it/s]\n",
      "100%|██████████| 107600/107600 [00:31<00:00, 3461.55it/s]/s]\n",
      " 54%|█████▎    | 4302793/8013769 [31:53<2:38:41, 389.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.22235083580017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 4398765/8013769 [33:16<01:38, 36748.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 81.60665225982666\n",
      "\n",
      "saving chunks @ 4400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2222/2222 [00:00<00:00, 3082.36it/s]\n",
      "100%|██████████| 108883/108883 [00:32<00:00, 3310.86it/s]\n",
      " 55%|█████▌    | 4407890/8013769 [33:51<4:52:28, 205.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 115.23376679420471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 4497794/8013769 [33:54<01:40, 34983.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.4923439025878906\n",
      "\n",
      "saving chunks @ 4500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2209/2209 [00:00<00:00, 3029.35it/s]\n",
      "100%|██████████| 108284/108284 [00:33<00:00, 3256.89it/s]/s]\n",
      " 56%|█████▌    | 4502858/8013769 [34:31<2:27:43, 396.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 36.48664093017578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4599190/8013769 [34:34<01:33, 36682.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.5848641395568848\n",
      "\n",
      "saving chunks @ 4600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2209/2209 [00:00<00:00, 3144.88it/s]\n",
      "100%|██████████| 108274/108274 [00:31<00:00, 3472.39it/s]/s]\n",
      " 57%|█████▋    | 4602956/8013769 [35:09<2:26:30, 388.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.48715281486511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 4698791/8013769 [35:12<01:29, 37124.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.815823793411255\n",
      "\n",
      "saving chunks @ 4700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [00:00<00:00, 3324.61it/s]\n",
      "100%|██████████| 108038/108038 [00:31<00:00, 3434.73it/s]/s]\n",
      " 59%|█████▊    | 4702814/8013769 [35:48<2:22:32, 387.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.95064926147461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 4797409/8013769 [35:51<01:27, 36968.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8006811141967773\n",
      "\n",
      "saving chunks @ 4800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:00<00:00, 3148.99it/s]\n",
      "100%|██████████| 107370/107370 [00:31<00:00, 3437.40it/s]/s]\n",
      " 60%|█████▉    | 4803395/8013769 [36:26<1:59:02, 449.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.74898433685303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 4899845/8013769 [36:29<01:24, 36674.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8246090412139893\n",
      "\n",
      "saving chunks @ 4900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2219/2219 [00:00<00:00, 3194.74it/s]\n",
      "100%|██████████| 108765/108765 [00:31<00:00, 3453.51it/s]/s]\n",
      " 61%|██████    | 4902787/8013769 [37:05<2:26:25, 354.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.02985453605652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4999970/8013769 [37:08<01:21, 37072.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.719108819961548\n",
      "\n",
      "saving chunks @ 5000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3411.45it/s]\n",
      "100%|██████████| 107488/107488 [00:31<00:00, 3419.97it/s]/s]\n",
      " 62%|██████▏   | 5003441/8013769 [37:44<2:11:16, 382.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.80859088897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 5099927/8013769 [37:46<01:18, 37199.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8254265785217285\n",
      "\n",
      "saving chunks @ 5100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2220/2220 [00:00<00:00, 3459.72it/s]\n",
      "100%|██████████| 108783/108783 [00:31<00:00, 3410.06it/s]/s]\n",
      " 64%|██████▎   | 5102881/8013769 [38:23<2:16:56, 354.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.38505291938782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 5198363/8013769 [38:25<01:15, 37171.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7836763858795166\n",
      "\n",
      "saving chunks @ 5200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 3607.00it/s]\n",
      "100%|██████████| 108458/108458 [00:31<00:00, 3420.69it/s]/s]\n",
      " 65%|██████▍   | 5203259/8013769 [39:02<1:52:57, 414.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.122440576553345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 5299741/8013769 [39:04<01:12, 37202.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8502707481384277\n",
      "\n",
      "saving chunks @ 5300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2220/2220 [00:00<00:00, 3632.32it/s]\n",
      "100%|██████████| 108808/108808 [00:32<00:00, 3400.03it/s]/s]\n",
      " 66%|██████▌   | 5302553/8013769 [39:41<2:10:44, 345.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.48035740852356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 5398914/8013769 [39:43<01:09, 37548.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7801899909973145\n",
      "\n",
      "saving chunks @ 5400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2184/2184 [00:00<00:00, 3628.53it/s]\n",
      "100%|██████████| 107064/107064 [00:31<00:00, 3403.68it/s]/s]\n",
      " 67%|██████▋   | 5403288/8013769 [40:19<1:47:13, 405.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.85517597198486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 5499298/8013769 [40:22<01:08, 36939.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7952518463134766\n",
      "\n",
      "saving chunks @ 5500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2214/2214 [00:00<00:00, 3451.82it/s]\n",
      "100%|██████████| 108487/108487 [00:31<00:00, 3497.22it/s]/s]\n",
      " 69%|██████▊   | 5502655/8013769 [40:57<1:52:55, 370.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.47511267662048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 5598619/8013769 [41:00<01:03, 37880.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7575271129608154\n",
      "\n",
      "saving chunks @ 5600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [00:00<00:00, 3142.43it/s]\n",
      "100%|██████████| 108044/108044 [00:30<00:00, 3522.49it/s]/s]\n",
      " 70%|██████▉   | 5603497/8013769 [41:35<1:32:51, 432.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.148969411849976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 5696622/8013769 [41:38<01:02, 37001.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.501344680786133\n",
      "\n",
      "saving chunks @ 5700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2216/2216 [00:00<00:00, 3451.89it/s]\n",
      "100%|██████████| 108597/108597 [00:31<00:00, 3395.88it/s]/s]\n",
      " 71%|███████   | 5706083/8013769 [42:14<1:01:23, 626.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.14048171043396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 5799086/8013769 [42:17<00:59, 37329.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.4699251651763916\n",
      "\n",
      "saving chunks @ 5800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:00<00:00, 3594.70it/s]\n",
      "100%|██████████| 107393/107393 [00:31<00:00, 3421.90it/s]/s]\n",
      " 72%|███████▏  | 5803410/8013769 [42:52<1:30:01, 409.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.48065948486328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 5899358/8013769 [42:55<00:57, 36844.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8689451217651367\n",
      "\n",
      "saving chunks @ 5900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [00:00<00:00, 3284.88it/s]\n",
      "100%|██████████| 108310/108310 [00:31<00:00, 3484.25it/s]/s]\n",
      " 74%|███████▎  | 5902594/8013769 [43:30<1:36:03, 366.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.64538311958313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 5998095/8013769 [43:33<00:54, 37275.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8160552978515625\n",
      "\n",
      "saving chunks @ 6000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [00:00<00:00, 3248.92it/s]\n",
      "100%|██████████| 107640/107640 [00:31<00:00, 3435.99it/s]/s]\n",
      " 75%|███████▍  | 6003497/8013769 [44:09<1:16:14, 439.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.83650541305542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 6099575/8013769 [44:11<00:51, 37066.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.911435604095459\n",
      "\n",
      "saving chunks @ 6100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [00:00<00:00, 3505.90it/s]\n",
      "100%|██████████| 107567/107567 [00:31<00:00, 3453.99it/s]/s]\n",
      " 76%|███████▌  | 6102733/8013769 [44:47<1:26:28, 368.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.701364040374756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 6197821/8013769 [44:49<00:48, 37216.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8570754528045654\n",
      "\n",
      "saving chunks @ 6200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2199/2199 [00:00<00:00, 3362.07it/s]\n",
      "100%|██████████| 107785/107785 [00:31<00:00, 3416.14it/s]/s]\n",
      " 77%|███████▋  | 6203708/8013769 [45:25<1:07:20, 447.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.08004069328308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 6299863/8013769 [45:28<00:46, 37064.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.794567346572876\n",
      "\n",
      "saving chunks @ 6300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2198/2198 [00:00<00:00, 3572.61it/s]\n",
      "100%|██████████| 107745/107745 [00:31<00:00, 3392.25it/s]/s]\n",
      " 79%|███████▊  | 6302871/8013769 [46:04<1:19:48, 357.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.190139293670654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 6396664/8013769 [47:27<00:43, 37159.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 81.3326678276062\n",
      "\n",
      "saving chunks @ 6400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2195/2195 [00:00<00:00, 3528.06it/s]\n",
      "100%|██████████| 107566/107566 [00:31<00:00, 3452.59it/s]\n",
      " 80%|███████▉  | 6408376/8013769 [48:01<1:54:34, 233.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 113.12678027153015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 6498176/8013769 [48:03<00:44, 34125.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.468920946121216\n",
      "\n",
      "saving chunks @ 6500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 3157.77it/s]\n",
      "100%|██████████| 108451/108451 [00:32<00:00, 3352.34it/s]/s]\n",
      " 81%|████████  | 6502871/8013769 [48:40<1:03:03, 399.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.538190603256226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 6597622/8013769 [48:42<00:39, 36099.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.5922133922576904\n",
      "\n",
      "saving chunks @ 6600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2189/2189 [00:00<00:00, 3551.57it/s]\n",
      "100%|██████████| 107265/107265 [00:31<00:00, 3452.31it/s]/s]\n",
      " 82%|████████▏ | 6607217/8013769 [49:17<35:13, 665.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.30036926269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 6699587/8013769 [49:20<00:35, 37177.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8370492458343506\n",
      "\n",
      "saving chunks @ 6700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2181/2181 [00:00<00:00, 3059.94it/s]\n",
      "100%|██████████| 106870/106870 [00:30<00:00, 3460.04it/s]/s]\n",
      " 84%|████████▎ | 6702864/8013769 [49:55<58:00, 376.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.45401191711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 6799049/8013769 [49:58<00:32, 37083.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 3.0644898414611816\n",
      "\n",
      "saving chunks @ 6800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [00:00<00:00, 3379.43it/s]\n",
      "100%|██████████| 107649/107649 [00:31<00:00, 3418.23it/s]/s]\n",
      " 85%|████████▍ | 6807163/8013769 [50:34<33:10, 606.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.225547790527344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6899425/8013769 [50:37<00:29, 37445.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.9567136764526367\n",
      "\n",
      "saving chunks @ 6900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204/2204 [00:00<00:00, 3586.33it/s]\n",
      "100%|██████████| 108028/108028 [00:31<00:00, 3472.22it/s]/s]\n",
      " 86%|████████▌ | 6902764/8013769 [51:12<49:42, 372.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.70098519325256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 6996274/8013769 [51:15<00:27, 37211.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.724414825439453\n",
      "\n",
      "saving chunks @ 7000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3133.54it/s]\n",
      "100%|██████████| 107471/107471 [00:31<00:00, 3417.62it/s]/s]\n",
      " 87%|████████▋ | 7003720/8013769 [51:51<33:49, 497.75it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.888914585113525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 7096307/8013769 [51:53<00:24, 37154.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8196020126342773\n",
      "\n",
      "saving chunks @ 7100000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3379.66it/s]\n",
      "100%|██████████| 107458/107458 [00:30<00:00, 3514.48it/s]/s]\n",
      " 89%|████████▊ | 7102803/8013769 [52:28<32:27, 467.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.06209969520569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 7199037/8013769 [52:31<00:21, 37383.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7814478874206543\n",
      "\n",
      "saving chunks @ 7200000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2194/2194 [00:00<00:00, 3240.73it/s]\n",
      "100%|██████████| 107533/107533 [00:30<00:00, 3517.59it/s]/s]\n",
      " 90%|████████▉ | 7203765/8013769 [53:06<31:22, 430.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.04829668998718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 7296221/8013769 [53:09<00:19, 37541.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.885383129119873\n",
      "\n",
      "saving chunks @ 7300000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2203/2203 [00:00<00:00, 3602.83it/s]\n",
      "100%|██████████| 107954/107954 [00:31<00:00, 3475.90it/s]/s]\n",
      " 91%|█████████ | 7302751/8013769 [53:44<25:36, 462.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.57092905044556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 7396941/8013769 [53:47<00:16, 37678.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.695134162902832\n",
      "\n",
      "saving chunks @ 7400000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2184/2184 [00:00<00:00, 3368.04it/s]\n",
      "100%|██████████| 107056/107056 [00:30<00:00, 3474.14it/s]/s]\n",
      " 92%|█████████▏| 7403647/8013769 [54:22<20:57, 485.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.17591714859009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 7499598/8013769 [54:25<00:13, 37065.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.800828456878662\n",
      "\n",
      "saving chunks @ 7500000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:00<00:00, 3185.24it/s]\n",
      "100%|██████████| 107392/107392 [00:31<00:00, 3412.11it/s]/s]\n",
      " 94%|█████████▎| 7502844/8013769 [55:01<23:18, 365.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.98054313659668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 7597663/8013769 [55:04<00:11, 36608.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.7611470222473145\n",
      "\n",
      "saving chunks @ 7600000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2198/2198 [00:00<00:00, 3140.93it/s]\n",
      "100%|██████████| 107720/107720 [00:31<00:00, 3458.13it/s]/s]\n",
      " 95%|█████████▍| 7607234/8013769 [55:39<10:20, 655.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.630077838897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 7699318/8013769 [55:42<00:08, 37106.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.803788185119629\n",
      "\n",
      "saving chunks @ 7700000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2212/2212 [00:00<00:00, 3641.71it/s]\n",
      "100%|██████████| 108416/108416 [00:30<00:00, 3519.93it/s]/s]\n",
      " 96%|█████████▌| 7700000/8013769 [56:17<19:31, 267.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.22974944114685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 7797294/8013769 [56:20<00:05, 37142.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.855628490447998\n",
      "\n",
      "saving chunks @ 7800000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2213/2213 [00:00<00:00, 3649.53it/s]\n",
      "100%|██████████| 108482/108482 [00:31<00:00, 3410.48it/s]/s]\n",
      " 97%|█████████▋| 7806930/8013769 [56:56<05:22, 641.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 35.287567138671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 7899446/8013769 [56:59<00:03, 36671.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.8178677558898926\n",
      "\n",
      "saving chunks @ 7900000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2193/2193 [00:00<00:00, 3507.93it/s]\n",
      "100%|██████████| 107458/107458 [00:30<00:00, 3577.52it/s]/s]\n",
      " 99%|█████████▊| 7908224/8013769 [57:33<02:34, 683.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 33.49688220024109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 7997733/8013769 [57:36<00:00, 36942.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize: 2.4149179458618164\n",
      "\n",
      "saving chunks @ 8000000th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [00:00<00:00, 3375.42it/s]\n",
      "100%|██████████| 108089/108089 [00:31<00:00, 3444.99it/s]/s]\n",
      "100%|█████████▉| 8007351/8013769 [58:11<00:09, 668.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save: 34.462011098861694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8011000/8013769 [58:11<00:02, 957.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving chunks @ 8013769th example mark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:00<00:00, 3661.54it/s]\n",
      "100%|██████████| 14771/14771 [00:04<00:00, 3457.95it/s]\n",
      "100%|█████████▉| 8013768/8013769 [58:16<00:00, 2291.79it/s]\n"
     ]
    }
   ],
   "source": [
    "ArrayRecordWriter = array_record_module.ArrayRecordWriter\n",
    "ArrayRecordReader = array_record_module.ArrayRecordReader\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "ds_output_file_train = \"./data_dir/openwebtext_splits_1024_train\"\n",
    "ds_output_file_eval = \"./data_dir/openwebtext_splits_1024_eval\"\n",
    "\n",
    "n_examples = 8013769  # tiny: 2; small: 10_000; full: 8013769\n",
    "save_every_examples = 100_000\n",
    "block_size = 1024  # size of the chunk\n",
    "\n",
    "# data_iter = iter(source)\n",
    "\n",
    "all_tokens = []\n",
    "count = 0\n",
    "count_per_save = 0\n",
    "eval_chunks = []\n",
    "\n",
    "writer_train = ArrayRecordWriter(ds_output_file_train, \"group_size:1\")\n",
    "writer_eval = ArrayRecordWriter(ds_output_file_eval, \"group_size:1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for tokens in tqdm(all_tokenized_data_input_ids):\n",
    "    # tokens = tokenizer(example[\"text\"])[\"input_ids\"]\n",
    "    all_tokens.extend(tokens + [tokenizer.eos_token_id])\n",
    "    count += 1\n",
    "    count_per_save += 1\n",
    "\n",
    "    # pause to save when having tokenized enough examples for saving.\n",
    "    time1 = time.time()\n",
    "    if count_per_save >= save_every_examples:\n",
    "        # save to disk\n",
    "        saved_length = (len(all_tokens) // block_size) * block_size\n",
    "        chunks = [\n",
    "            all_tokens[i : i + block_size] for i in range(0, saved_length, block_size)\n",
    "        ]\n",
    "\n",
    "        print(\"Time taken to tokenize:\", time.time() - time1)\n",
    "        print(f\"\\nsaving chunks @ {count}th example mark...\")\n",
    "        np.random.shuffle(chunks)\n",
    "        num_eval = int(len(chunks) * 0.02)  # put 2% of chunks into eval split.\n",
    "        for eval_i in tqdm(range(num_eval)):\n",
    "            feature = {\n",
    "                \"text\": _int64_feature(chunks[eval_i]),\n",
    "            }\n",
    "            example_proto = tf.train.Example(\n",
    "                features=tf.train.Features(feature=feature)\n",
    "            )\n",
    "            writer_eval.write(example_proto.SerializeToString())\n",
    "\n",
    "        for train_i in tqdm(range(num_eval, len(chunks))):\n",
    "            feature = {\n",
    "                \"text\": _int64_feature(chunks[train_i]),\n",
    "            }\n",
    "            example_proto = tf.train.Example(\n",
    "                features=tf.train.Features(feature=feature)\n",
    "            )\n",
    "            writer_train.write(example_proto.SerializeToString())\n",
    "        print(\"Time taken to save:\", time.time() - time1)\n",
    "        # prepare for the next round of tokenize-n-save.\n",
    "        all_tokens = all_tokens[saved_length:]\n",
    "        count_per_save = 0\n",
    "\n",
    "    # stop when having tokenized enough examples for total #.\n",
    "    if count >= n_examples:\n",
    "        # save to disk\n",
    "        saved_length = (len(all_tokens) // block_size) * block_size\n",
    "        chunks = [\n",
    "            all_tokens[i : i + block_size] for i in range(0, saved_length, block_size)\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nsaving chunks @ {count}th example mark...\")\n",
    "        np.random.shuffle(chunks)\n",
    "        num_eval = int(len(chunks) * 0.02)  # put 2% of chunks into eval split.\n",
    "        for eval_i in tqdm(range(num_eval)):\n",
    "            feature = {\n",
    "                \"text\": _int64_feature(chunks[eval_i]),\n",
    "            }\n",
    "            example_proto = tf.train.Example(\n",
    "                features=tf.train.Features(feature=feature)\n",
    "            )\n",
    "            writer_eval.write(example_proto.SerializeToString())\n",
    "\n",
    "        for train_i in tqdm(range(num_eval, len(chunks))):\n",
    "            feature = {\n",
    "                \"text\": _int64_feature(chunks[train_i]),\n",
    "            }\n",
    "            example_proto = tf.train.Example(\n",
    "                features=tf.train.Features(feature=feature)\n",
    "            )\n",
    "            writer_train.write(example_proto.SerializeToString())\n",
    "        break\n",
    "\n",
    "writer_train.close()\n",
    "writer_eval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [13924,\n",
       "  12,\n",
       "  559,\n",
       "  12,\n",
       "  35784,\n",
       "  11,\n",
       "  25051,\n",
       "  357,\n",
       "  18474,\n",
       "  8,\n",
       "  1377,\n",
       "  45591,\n",
       "  4970,\n",
       "  11,\n",
       "  1319,\n",
       "  44556,\n",
       "  287,\n",
       "  2356,\n",
       "  290,\n",
       "  44787,\n",
       "  379,\n",
       "  1204,\n",
       "  11,\n",
       "  7342,\n",
       "  7519,\n",
       "  290,\n",
       "  20669,\n",
       "  2513,\n",
       "  1497,\n",
       "  422,\n",
       "  257,\n",
       "  2214,\n",
       "  4436,\n",
       "  3217,\n",
       "  1755,\n",
       "  706,\n",
       "  257,\n",
       "  21402,\n",
       "  3315,\n",
       "  1074,\n",
       "  23724,\n",
       "  262,\n",
       "  1989,\n",
       "  11,\n",
       "  2282,\n",
       "  340,\n",
       "  373,\n",
       "  5213,\n",
       "  546,\n",
       "  2324,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  464,\n",
       "  2551,\n",
       "  1364,\n",
       "  8100,\n",
       "  5953,\n",
       "  8366,\n",
       "  34428,\n",
       "  298,\n",
       "  2986,\n",
       "  33708,\n",
       "  42095,\n",
       "  355,\n",
       "  262,\n",
       "  691,\n",
       "  6253,\n",
       "  379,\n",
       "  262,\n",
       "  4436,\n",
       "  284,\n",
       "  651,\n",
       "  262,\n",
       "  3871,\n",
       "  832,\n",
       "  262,\n",
       "  1755,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  18474,\n",
       "  7317,\n",
       "  2098,\n",
       "  11,\n",
       "  1912,\n",
       "  319,\n",
       "  10275,\n",
       "  351,\n",
       "  617,\n",
       "  286,\n",
       "  262,\n",
       "  7519,\n",
       "  11,\n",
       "  326,\n",
       "  262,\n",
       "  1578,\n",
       "  7973,\n",
       "  6149,\n",
       "  262,\n",
       "  21402,\n",
       "  3274,\n",
       "  22225,\n",
       "  290,\n",
       "  7929,\n",
       "  4816,\n",
       "  284,\n",
       "  36316,\n",
       "  13,\n",
       "  2102,\n",
       "  11,\n",
       "  21402,\n",
       "  5953,\n",
       "  36831,\n",
       "  2269,\n",
       "  861,\n",
       "  402,\n",
       "  2926,\n",
       "  82,\n",
       "  11,\n",
       "  257,\n",
       "  6253,\n",
       "  508,\n",
       "  373,\n",
       "  379,\n",
       "  262,\n",
       "  4436,\n",
       "  351,\n",
       "  3126,\n",
       "  21402,\n",
       "  3315,\n",
       "  8213,\n",
       "  11,\n",
       "  531,\n",
       "  340,\n",
       "  373,\n",
       "  465,\n",
       "  2551,\n",
       "  284,\n",
       "  2834,\n",
       "  262,\n",
       "  1074,\n",
       "  503,\n",
       "  329,\n",
       "  262,\n",
       "  1755,\n",
       "  13,\n",
       "  402,\n",
       "  2926,\n",
       "  82,\n",
       "  531,\n",
       "  339,\n",
       "  9167,\n",
       "  471,\n",
       "  13,\n",
       "  45,\n",
       "  13,\n",
       "  2324,\n",
       "  8213,\n",
       "  284,\n",
       "  3085,\n",
       "  262,\n",
       "  4436,\n",
       "  13417,\n",
       "  11,\n",
       "  475,\n",
       "  373,\n",
       "  1297,\n",
       "  326,\n",
       "  4167,\n",
       "  24952,\n",
       "  561,\n",
       "  691,\n",
       "  307,\n",
       "  1498,\n",
       "  284,\n",
       "  36316,\n",
       "  262,\n",
       "  1074,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1544,\n",
       "  531,\n",
       "  340,\n",
       "  373,\n",
       "  257,\n",
       "  366,\n",
       "  83,\n",
       "  619,\n",
       "  2551,\n",
       "  1,\n",
       "  475,\n",
       "  326,\n",
       "  339,\n",
       "  6292,\n",
       "  262,\n",
       "  471,\n",
       "  13,\n",
       "  45,\n",
       "  13,\n",
       "  2897,\n",
       "  284,\n",
       "  36316,\n",
       "  706,\n",
       "  257,\n",
       "  5398,\n",
       "  3315,\n",
       "  1074,\n",
       "  11,\n",
       "  635,\n",
       "  379,\n",
       "  262,\n",
       "  4436,\n",
       "  351,\n",
       "  5398,\n",
       "  2324,\n",
       "  3790,\n",
       "  11,\n",
       "  1364,\n",
       "  262,\n",
       "  2524,\n",
       "  3217,\n",
       "  6672,\n",
       "  13,\n",
       "  383,\n",
       "  21402,\n",
       "  1074,\n",
       "  4504,\n",
       "  3909,\n",
       "  3329,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  38,\n",
       "  2926,\n",
       "  82,\n",
       "  531,\n",
       "  262,\n",
       "  1578,\n",
       "  7973,\n",
       "  468,\n",
       "  4987,\n",
       "  284,\n",
       "  2148,\n",
       "  2324,\n",
       "  329,\n",
       "  3909,\n",
       "  1755,\n",
       "  13,\n",
       "  383,\n",
       "  1074,\n",
       "  468,\n",
       "  9167,\n",
       "  262,\n",
       "  21402,\n",
       "  1230,\n",
       "  284,\n",
       "  3758,\n",
       "  663,\n",
       "  898,\n",
       "  6553,\n",
       "  329,\n",
       "  262,\n",
       "  2214,\n",
       "  4436,\n",
       "  11,\n",
       "  543,\n",
       "  402,\n",
       "  2926,\n",
       "  82,\n",
       "  13423,\n",
       "  284,\n",
       "  9240,\n",
       "  2739,\n",
       "  3502,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  19309,\n",
       "  42703,\n",
       "  284,\n",
       "  262,\n",
       "  8100,\n",
       "  989,\n",
       "  326,\n",
       "  42095,\n",
       "  373,\n",
       "  262,\n",
       "  691,\n",
       "  6253,\n",
       "  1364,\n",
       "  379,\n",
       "  262,\n",
       "  4347,\n",
       "  12,\n",
       "  559,\n",
       "  12,\n",
       "  35784,\n",
       "  2214,\n",
       "  4436,\n",
       "  11,\n",
       "  471,\n",
       "  13,\n",
       "  45,\n",
       "  13,\n",
       "  6523,\n",
       "  5780,\n",
       "  399,\n",
       "  274,\n",
       "  343,\n",
       "  2584,\n",
       "  531,\n",
       "  3909,\n",
       "  326,\n",
       "  262,\n",
       "  995,\n",
       "  1767,\n",
       "  338,\n",
       "  4365,\n",
       "  287,\n",
       "  25051,\n",
       "  750,\n",
       "  407,\n",
       "  1502,\n",
       "  597,\n",
       "  3315,\n",
       "  1074,\n",
       "  284,\n",
       "  2666,\n",
       "  13,\n",
       "  1002,\n",
       "  262,\n",
       "  1074,\n",
       "  1364,\n",
       "  11,\n",
       "  340,\n",
       "  373,\n",
       "  379,\n",
       "  262,\n",
       "  2581,\n",
       "  286,\n",
       "  511,\n",
       "  898,\n",
       "  4009,\n",
       "  11,\n",
       "  339,\n",
       "  531,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  7407,\n",
       "  6327,\n",
       "  8252,\n",
       "  1616,\n",
       "  11,\n",
       "  262,\n",
       "  471,\n",
       "  13,\n",
       "  45,\n",
       "  13,\n",
       "  8796,\n",
       "  7705,\n",
       "  2276,\n",
       "  329,\n",
       "  4167,\n",
       "  19934,\n",
       "  4560,\n",
       "  11,\n",
       "  1297,\n",
       "  7638,\n",
       "  1568,\n",
       "  326,\n",
       "  1957,\n",
       "  2324,\n",
       "  3790,\n",
       "  10762,\n",
       "  262,\n",
       "  34064,\n",
       "  4436,\n",
       "  21596,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1,\n",
       "  1026,\n",
       "  2331,\n",
       "  326,\n",
       "  356,\n",
       "  1053,\n",
       "  2982,\n",
       "  617,\n",
       "  3136,\n",
       "  287,\n",
       "  262,\n",
       "  3230,\n",
       "  2056,\n",
       "  326,\n",
       "  262,\n",
       "  1578,\n",
       "  7973,\n",
       "  1965,\n",
       "  393,\n",
       "  4137,\n",
       "  617,\n",
       "  3315,\n",
       "  3466,\n",
       "  284,\n",
       "  407,\n",
       "  670,\n",
       "  597,\n",
       "  517,\n",
       "  287,\n",
       "  617,\n",
       "  15760,\n",
       "  1377,\n",
       "  326,\n",
       "  318,\n",
       "  407,\n",
       "  2081,\n",
       "  11,\n",
       "  326,\n",
       "  318,\n",
       "  3190,\n",
       "  35212,\n",
       "  553,\n",
       "  8252,\n",
       "  1616,\n",
       "  531,\n",
       "  3909,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  18474,\n",
       "  2008,\n",
       "  422,\n",
       "  262,\n",
       "  3715,\n",
       "  3217,\n",
       "  1755,\n",
       "  2523,\n",
       "  262,\n",
       "  21402,\n",
       "  1074,\n",
       "  24157,\n",
       "  510,\n",
       "  663,\n",
       "  9416,\n",
       "  290,\n",
       "  4305,\n",
       "  351,\n",
       "  281,\n",
       "  27675,\n",
       "  286,\n",
       "  4171,\n",
       "  12,\n",
       "  2978,\n",
       "  4164,\n",
       "  276,\n",
       "  471,\n",
       "  13,\n",
       "  45,\n",
       "  13,\n",
       "  4167,\n",
       "  24952,\n",
       "  287,\n",
       "  7498,\n",
       "  13960,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  7680,\n",
       "  393,\n",
       "  751,\n",
       "  284,\n",
       "  8100,\n",
       "  338,\n",
       "  6831,\n",
       "  286,\n",
       "  4814,\n",
       "  6506,\n",
       "  287,\n",
       "  25051,\n",
       "  198,\n",
       "  198,\n",
       "  8205,\n",
       "  32283,\n",
       "  1377,\n",
       "  18419,\n",
       "  416,\n",
       "  584,\n",
       "  8100,\n",
       "  23033,\n",
       "  11,\n",
       "  2324,\n",
       "  8213,\n",
       "  290,\n",
       "  379,\n",
       "  1551,\n",
       "  530,\n",
       "  48723,\n",
       "  15849,\n",
       "  508,\n",
       "  6520,\n",
       "  284,\n",
       "  2666,\n",
       "  1377,\n",
       "  15276,\n",
       "  262,\n",
       "  2476,\n",
       "  286,\n",
       "  262,\n",
       "  1679,\n",
       "  3871,\n",
       "  11,\n",
       "  475,\n",
       "  612,\n",
       "  373,\n",
       "  1310,\n",
       "  484,\n",
       "  714,\n",
       "  466,\n",
       "  1231,\n",
       "  9416,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  5167,\n",
       "  661,\n",
       "  11,\n",
       "  617,\n",
       "  287,\n",
       "  4688,\n",
       "  4006,\n",
       "  11,\n",
       "  547,\n",
       "  6908,\n",
       "  1359,\n",
       "  287,\n",
       "  2739,\n",
       "  3217,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1,\n",
       "  40,\n",
       "  1053,\n",
       "  1239,\n",
       "  587,\n",
       "  287,\n",
       "  257,\n",
       "  3074,\n",
       "  588,\n",
       "  428,\n",
       "  13,\n",
       "  770,\n",
       "  318,\n",
       "  2407,\n",
       "  11441,\n",
       "  553,\n",
       "  42095,\n",
       "  531,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  3152,\n",
       "  257,\n",
       "  390,\n",
       "  11999,\n",
       "  286,\n",
       "  3315,\n",
       "  7291,\n",
       "  287,\n",
       "  25051,\n",
       "  338,\n",
       "  3139,\n",
       "  11,\n",
       "  18907,\n",
       "  1817,\n",
       "  550,\n",
       "  12062,\n",
       "  2073,\n",
       "  284,\n",
       "  1011,\n",
       "  3871,\n",
       "  11,\n",
       "  617,\n",
       "  286,\n",
       "  4150,\n",
       "  550,\n",
       "  6989,\n",
       "  6049,\n",
       "  14649,\n",
       "  1377,\n",
       "  40338,\n",
       "  602,\n",
       "  290,\n",
       "  1182,\n",
       "  6821,\n",
       "  1377,\n",
       "  739,\n",
       "  262,\n",
       "  29632,\n",
       "  13,\n",
       "  12691,\n",
       "  550,\n",
       "  6989,\n",
       "  257,\n",
       "  1049,\n",
       "  1730,\n",
       "  286,\n",
       "  2910,\n",
       "  2994,\n",
       "  11,\n",
       "  475,\n",
       "  612,\n",
       "  547,\n",
       "  645,\n",
       "  2910,\n",
       "  9416,\n",
       "  1364,\n",
       "  379,\n",
       "  262,\n",
       "  15760,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  8205,\n",
       "  32283,\n",
       "  15240,\n",
       "  326,\n",
       "  617,\n",
       "  561,\n",
       "  407,\n",
       "  7866,\n",
       "  262,\n",
       "  1755,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1544,\n",
       "  290,\n",
       "  262,\n",
       "  1854,\n",
       "  9658,\n",
       "  351,\n",
       "  262,\n",
       "  6686,\n",
       "  477,\n",
       "  1755,\n",
       "  11,\n",
       "  706,\n",
       "  262,\n",
       "  3315,\n",
       "  1074,\n",
       "  550,\n",
       "  1364,\n",
       "  290,\n",
       "  706,\n",
       "  262,\n",
       "  27298,\n",
       "  2921,\n",
       "  503,\n",
       "  290,\n",
       "  262,\n",
       "  29804,\n",
       "  2900,\n",
       "  7078,\n",
       "  2042,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  8205,\n",
       "  32283,\n",
       "  20738,\n",
       "  3871,\n",
       "  6,\n",
       "  9204,\n",
       "  5895,\n",
       "  11,\n",
       "  17169,\n",
       "  2356,\n",
       "  43492,\n",
       "  290,\n",
       "  3767,\n",
       "  45840,\n",
       "  516,\n",
       "  1454,\n",
       "  862,\n",
       "  13,\n",
       "  679,\n",
       "  44945,\n",
       "  1115,\n",
       "  649,\n",
       "  3871,\n",
       "  287,\n",
       "  4688,\n",
       "  4006,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  2953,\n",
       "  513,\n",
       "  25,\n",
       "  2231,\n",
       "  257,\n",
       "  13,\n",
       "  76,\n",
       "  1539,\n",
       "  339,\n",
       "  4481,\n",
       "  257,\n",
       "  3275,\n",
       "  319,\n",
       "  3009,\n",
       "  25,\n",
       "  366,\n",
       "  31216,\n",
       "  278,\n",
       "  477,\n",
       "  299,\n",
       "  4799,\n",
       "  379,\n",
       "  387,\n",
       "  8846,\n",
       "  2214,\n",
       "  10496,\n",
       "  13,\n",
       "  6041,\n",
       "  286,\n",
       "  670,\n",
       "  11,\n",
       "  475,\n",
       "  477,\n",
       "  3871,\n",
       "  8245,\n",
       "  13,\n",
       "  2900,\n",
       "  616,\n",
       "  5462,\n",
       "  656,\n",
       "  257,\n",
       "  8469,\n",
       "  1117,\n",
       "  1074,\n",
       "  9975,\n",
       "  526,\n",
       "  198,\n",
       "  198,\n",
       "  8491,\n",
       "  345,\n",
       "  287,\n",
       "  25051,\n",
       "  290,\n",
       "  3338,\n",
       "  30,\n",
       "  8734,\n",
       "  534,\n",
       "  5205,\n",
       "  198,\n",
       "  198,\n",
       "  1544,\n",
       "  531,\n",
       "  262,\n",
       "  21402,\n",
       "  7519,\n",
       "  750,\n",
       "  407,\n",
       "  765,\n",
       "  284,\n",
       "  2666,\n",
       "  511,\n",
       "  3871,\n",
       "  2157,\n",
       "  475,\n",
       "  547,\n",
       "  6149,\n",
       "  503,\n",
       "  416,\n",
       "  262,\n",
       "  1578,\n",
       "  7973,\n",
       "  11,\n",
       "  543,\n",
       "  1908,\n",
       "  16893,\n",
       "  284,\n",
       "  4839,\n",
       "  606,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1,\n",
       "  1858,\n",
       "  318,\n",
       "  2328,\n",
       "  546,\n",
       "  23766,\n",
       "  407,\n",
       "  1290,\n",
       "  422,\n",
       "  994,\n",
       "  1377,\n",
       "  290,\n",
       "  428,\n",
       "  318,\n",
       "  636,\n",
       "  286,\n",
       "  262,\n",
       "  1917,\n",
       "  553,\n",
       "  42095,\n",
       "  531,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1858,\n",
       "  423,\n",
       "  587,\n",
       "  16830,\n",
       "  3136,\n",
       "  286,\n",
       "  3685,\n",
       "  3690,\n",
       "  262,\n",
       "  3139,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1,\n",
       "  2061,\n",
       "  318,\n",
       "  8871,\n",
       "  284,\n",
       "  502,\n",
       "  355,\n",
       "  257,\n",
       "  14325,\n",
       "  318,\n",
       "  326,\n",
       "  3871,\n",
       "  508,\n",
       "  655,\n",
       "  550,\n",
       "  8185,\n",
       "  11,\n",
       "  3871,\n",
       "  508,\n",
       "  389,\n",
       "  19475,\n",
       "  2801,\n",
       "  11,\n",
       "  389,\n",
       "  6986,\n",
       "  852,\n",
       "  1364,\n",
       "  994,\n",
       "  11,\n",
       "  8168,\n",
       "  284,\n",
       "  1337,\n",
       "  329,\n",
       "  606,\n",
       "  553,\n",
       "  42095,\n",
       "  531,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  50,\n",
       "  15918,\n",
       "  21204,\n",
       "  11,\n",
       "  257,\n",
       "  48723,\n",
       "  508,\n",
       "  468,\n",
       "  587,\n",
       "  5742,\n",
       "  379,\n",
       "  262,\n",
       "  34064,\n",
       "  4436,\n",
       "  11,\n",
       "  531,\n",
       "  262,\n",
       "  3315,\n",
       "  3085,\n",
       "  1718,\n",
       "  749,\n",
       "  286,\n",
       "  262,\n",
       "  9416,\n",
       "  351,\n",
       "  606,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  1,\n",
       "  3237,\n",
       "  262,\n",
       "  7519,\n",
       "  11,\n",
       "  477,\n",
       "  262,\n",
       "  20669,\n",
       "  389,\n",
       "  3750,\n",
       "  553,\n",
       "  673,\n",
       "  531,\n",
       "  13,\n",
       "  366,\n",
       "  2990,\n",
       "  389,\n",
       "  2938,\n",
       "  284,\n",
       "  307,\n",
       "  736,\n",
       "  9439,\n",
       "  13,\n",
       "  1119,\n",
       "  550,\n",
       "  645,\n",
       "  1410,\n",
       "  319,\n",
       "  4305,\n",
       "  9975,\n",
       "  13,\n",
       "  632,\n",
       "  373,\n",
       "  281,\n",
       "  1502,\n",
       "  326,\n",
       "  1625,\n",
       "  6451,\n",
       "  526,\n",
       "  198,\n",
       "  198,\n",
       "  3347,\n",
       "  1297,\n",
       "  42095,\n",
       "  11,\n",
       "  366,\n",
       "  1026,\n",
       "  338,\n",
       "  655,\n",
       "  345,\n",
       "  526,\n",
       "  198,\n",
       "  198,\n",
       "  32,\n",
       "  767,\n",
       "  13,\n",
       "  15,\n",
       "  14735,\n",
       "  16295,\n",
       "  45096,\n",
       "  25051,\n",
       "  338,\n",
       "  3139,\n",
       "  1748,\n",
       "  3431,\n",
       "  6672,\n",
       "  11,\n",
       "  13891,\n",
       "  355,\n",
       "  867,\n",
       "  355,\n",
       "  513,\n",
       "  1510,\n",
       "  661,\n",
       "  355,\n",
       "  340,\n",
       "  277,\n",
       "  3577,\n",
       "  503,\n",
       "  1973,\n",
       "  262,\n",
       "  7022,\n",
       "  3277,\n",
       "  13,\n",
       "  40280,\n",
       "  286,\n",
       "  4138,\n",
       "  286,\n",
       "  661,\n",
       "  389,\n",
       "  15240,\n",
       "  2636,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  39,\n",
       "  4548,\n",
       "  72,\n",
       "  11,\n",
       "  262,\n",
       "  27335,\n",
       "  3277,\n",
       "  ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
